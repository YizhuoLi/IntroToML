{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Introduction to Machine Learning</center> #  \n",
    "## <center>ProblemSet4</center> ##  \n",
    "#### <center>Yizhuo Li</center> ####  \n",
    "#### <center>2019/03/10</center> ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Artificial Intelligence (7 points)**  \n",
    "(a) Describe what is computer vision and how it relates to machine learning.  \n",
    "(b) Describe what are low-level, mid-level, and high-level vision features and give two examples for each (6 examples in total).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (a) For me computer version is the algorithorm that can recognize the real world things in a picture.  \n",
    "    So, the realtion of it to the Machine learning is computer version use the algorithorms come from or related to the Machine learning. And it is the these algorithorms like unsuperivised learning algorithorms that help to recognize the things in a picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (b)\n",
    "    low-level features are the basic geometry graphs in a picture. Eg: lines, dots.  \n",
    "    mid-level features are what the low-level features form. It can be recognized as a small part of a big object. Eg: forms, color.\n",
    "    hich-level features are the features we can recognize or describe easily and are formed by the mid-level features. Eg: emothions, objects    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Artificial Neurons (10 points)**  \n",
    "(a) Describe what is Perceptron and Adaline and discuss what makes them similar as well as what makes them different.  \n",
    "(b) Show the mathematical steps of learning a Perceptron model over three epochs, using the training data shown in Table 1, model weights initialized to 0, and learning rate of 0.2. For full credit, you must include the mathematical steps used to derive the weights and three tables showing weights at each training update round (4 rows per table) that indicate all weights used for each epoch.  \n",
    "Sample | X1 | X2 | X3 | Y\n",
    "------------ | ------------- | ------------ | ------------ | ------------ \n",
    "1 | 0 | 0 | 0 | -1 \n",
    "2 | 1 | 0 | 0 | 1 \n",
    "3 | 1 | 1 | 0 | 1 \n",
    "4 | 0 | 1 | 1 | -1\n",
    "\n",
    "<center>Table 1: Data.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (a) The percepthon and the Adaline are both the ways try to use the linear regression to simulate the Neural work of human.   \n",
    "    They both use the training data to change the weights of each feature and try to get the perfect predict of the outcome. I think the both assume that the data is linear seperable. And the reault should be influenced by the original weights.  \n",
    "    What's the different is the perception will updata the weights for every sample but the Adaline update weights with accumulated \"weight update\" values. So, it updates based on continuous valued prediction rather than integer predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (b)The original weights is 0 and the learning rate = 0.2 Based on the equation:  \n",
    "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
    "\n",
    "    \\\\(W_{j} = W_{j} + \\eta*(target^{(i)}-outpute^{(i)})*x_{j}^{(i)}\\\\)  \n",
    "\n",
    "    we will update the weights.   \n",
    "    After we get the weights, we use the weights to calculate the predicted value and then use the value to update the weights.  \n",
    "    The equation of caltulate the predicted value is \\\\(W^{T}x\\\\)  \n",
    "    When \\\\(W^{T}x\\\\) >= 0, Predicted value is 1. Otherwise, the value is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    So, the predicted value is \n",
    "Predicted value | \n",
    "------------ |\n",
    "1 \n",
    "-1 \n",
    "1 \n",
    "1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The weights is\n",
    "W0 | W1 | W2 | W3\n",
    "------------ | ------------- | ------------ | ------------\n",
    " 0 | 0 | 0 | 0 \n",
    " -0.4 | 0 | 0 | 0\n",
    " 0 | 0.4 | 0 | 0 \n",
    " 0 | 0.4 | 0 | 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The first round calculate is  \n",
    "\\\\(\\Delta W_{0}= 0.2*(-1-1) = -0.4 \\\\)  \n",
    "\\\\(\\Delta W_{1}= 0.2*(-1-1)*0 = 0 \\\\)  \n",
    "\\\\(\\Delta W_{2}= 0.2*(-1-1)*0 = 0 \\\\)  \n",
    "\\\\(\\Delta W_{3}= 0.2*(-1-1)*0 = 0 \\\\)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Second round calculate is  \n",
    "\\\\(\\Delta W_{0}= 0.2*(1-(-1)) = 0.4 \\\\)  \n",
    "\\\\(\\Delta W_{0}= 0.2*(1-(-1))*1 = 0.4 \\\\)  \n",
    "\\\\(\\Delta W_{0}= 0.2*(1-(-1))*0 = 0 \\\\)  \n",
    "\\\\(\\Delta W_{0}= 0.2*(1-(-1))*0 = 0 \\\\)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The third round:\n",
    "    Because the output is same with the predicted, so the target equals to the outcome. So, the weights will not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Gradient Descent (8 points)**  \n",
    "(a) Describe what is stochastic gradient descent, batch gradient descent, and mini-batch gradient descent and identify what makes them different from each other.  \n",
    "(b) What gradient descent approach does Perceptron use?  \n",
    "(c) What gradient descent approach does Adaline use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    (a)All three ways are the Gradient Descent ways to find the local optimal solutions. The main different of them is the training dataset they use.  \n",
    "\n",
    "    Stochastic Gradient Descent (SGD):   \n",
    "    For each step (update), use calculations from one training example  \n",
    "    Adv: fast, can train for huge datasets  \n",
    "    Cons: Updates will bounce a lot.  \n",
    "    (b)Perceptron use this.  \n",
    "\n",
    "    Batch Gradient Descent (BGD)  \n",
    "    Each update calculations over all training examples  \n",
    "    (c)Adaline use this  \n",
    "\n",
    "    Mini-batch Gradient Descent  \n",
    "    Use the subset of training examples  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
