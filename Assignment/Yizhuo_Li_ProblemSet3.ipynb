{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clean-Up #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset | type_Comedy | type_Drama | length_Short | length_Medium | length_Long | IMDb Rating\n",
    "------------ | ------------- | ------------ | ------------ | ------------ | ------------ | ------------\n",
    "Train | 1  | 0  | 1  | 0  | 0  | 7.0\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 8.9\n",
    "Train | 0  | 1  | 1  | 0  | 0  | 4.2\n",
    "Train | 1  | 0  | 0  | 1  | 0  | 7.8\n",
    "Train | 0  | 1  | 0  | 0  | 1  | 8.5\n",
    "Train | 1  | 0  | 1  | 0  | 0  | 5.1\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 8.3\n",
    "Train | null  | null  | 1  | 0  | 0  | 6.6\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 7.7\n",
    "Train | 1  | 0  | 0  | 0  | 1  | 3.2\n",
    "Train | 0  | 1  | mull  | null  | null  | null\n",
    "Test | 1  | 0  | 0  | 0  | 1  | 8.6\n",
    "Test | 0  | 1  | 1  | 0  | 0  | null\n",
    "Test |  null | null  | 0  | 1  | 0  | 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For type and length, I am not sure what to do. Because it seems not good to use the mode to pedcitc every missing value.  \n",
    "\n",
    "For IMDb Rating:  \n",
    "get the average of the Training data: (7.0 + 8.9 + ... +3.2)/10 = 6.73  \n",
    "So, the result should be like this:  \n",
    "(At here we just use the average of all the training data and ignore the type of the movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset | type_Comedy | type_Drama | length_Short | length_Medium | length_Long | IMDb Rating\n",
    "------------ | ------------- | ------------ | ------------ | ------------ | ------------ | ------------\n",
    "Train | 1  | 0  | 1  | 0  | 0  | 7.0\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 8.9\n",
    "Train | 0  | 1  | 1  | 0  | 0  | 4.2\n",
    "Train | 1  | 0  | 0  | 1  | 0  | 7.8\n",
    "Train | 0  | 1  | 0  | 0  | 1  | 8.5\n",
    "Train | 1  | 0  | 1  | 0  | 0  | 5.1\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 8.3\n",
    "Train | null  | null  | 1  | 0  | 0  | 6.6\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 7.7\n",
    "Train | 1  | 0  | 0  | 0  | 1  | 3.2\n",
    "Train | 0  | 1  | mull  | null  | null  | 6.73\n",
    "Test | 1  | 0  | 0  | 0  | 1  | 8.6\n",
    "Test | 0  | 1  | 1  | 0  | 0  | 6.73\n",
    "Test |  null | null  | 0  | 1  | 0  | 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min-max scaling #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
    "\\\\(X_{max} - X_{min}\\\\) = 8.9 - 3.2 = 5.7  \n",
    "\n",
    "\\\\(X_{norm}^{(i)} = \\frac{X^{(i)}-X_{min}}{X_{max} - X_{min}}\\\\)  \n",
    "\n",
    "So, the IMDb Rating after min-max scaling is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset | type_Comedy | type_Drama | length_Short | length_Medium | length_Long | IMDb Rating | IMDb_max-min\n",
    "------------ | ------------- | ------------ | ------------ | ------------ | ------------ | ------------ | ------------\n",
    "Train | 1  | 0  | 1  | 0  | 0  | 7.0 | 0.67\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 8.9 | 1.0\n",
    "Train | 0  | 1  | 1  | 0  | 0  | 4.2 | 0.18\n",
    "Train | 1  | 0  | 0  | 1  | 0  | 7.8 | 0.81\n",
    "Train | 0  | 1  | 0  | 0  | 1  | 8.5 | 0.93\n",
    "Train | 1  | 0  | 1  | 0  | 0  | 5.1 | 0.33\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 8.3 | 0.89\n",
    "Train | null  | null  | 1  | 0  | 0  | 6.6 | 0.60\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 7.7 | 0.79\n",
    "Train | 1  | 0  | 0  | 0  | 1  | 3.2 | 0.0\n",
    "Train | 0  | 1  | mull  | null  | null  | 6.73 | 0.62\n",
    "Test | 1  | 0  | 0  | 0  | 1  | 8.6 | 0.95\n",
    "Test | 0  | 1  | 1  | 0  | 0  | 6.73 | 0.62\n",
    "Test |  null | null  | 0  | 1  | 0  | 7.0 | 0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standard deviation will be calculated based on training data.  \n",
    "\n",
    "\\\\(\\mu_{x} = 6.73\\\\)  \n",
    "\n",
    "\\\\(\\sigma_{x}\\\\) = 1.76\n",
    "\n",
    "\\\\(X_{std}^{(i)} = \\frac{X^{(i)}-\\mu_{x}}{\\sigma_{x}}\\\\)  \n",
    "\n",
    "So, the IMDb Rating after standardization scaling is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset | type_Comedy | type_Drama | length_Short | length_Medium | length_Long | IMDb Rating | IMDb_max-min | IMDb_std\n",
    "------------ | ------------- | ------------ | ------------ | ------------ | ------------ | ------------ | ------------ | ------------\n",
    "Train | 1  | 0  | 1  | 0  | 0  | 7.0 | 0.67 | 0.15\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 8.9 | 1.0 | 1.23\n",
    "Train | 0  | 1  | 1  | 0  | 0  | 4.2 | 0.18 | -1.4\n",
    "Train | 1  | 0  | 0  | 1  | 0  | 7.8 | 0.81 | 0.61\n",
    "Train | 0  | 1  | 0  | 0  | 1  | 8.5 | 0.93 | 1.00\n",
    "Train | 1  | 0  | 1  | 0  | 0  | 5.1 | 0.33 | -0.93\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 8.3 | 0.89 | 0.89\n",
    "Train | null  | null  | 1  | 0  | 0  | 6.6 | 0.60 | 0.07\n",
    "Train | 0  | 1  | 0  | 1  | 0  | 7.7 | 0.79 | 0.55\n",
    "Train | 1  | 0  | 0  | 0  | 1  | 3.2 | 0.0 | -2.00\n",
    "Train | 0  | 1  | mull  | null  | null  | 6.73 | 0.62 | 0\n",
    "Test | 1  | 0  | 0  | 0  | 1  | 8.6 | 0.95 | 1.06\n",
    "Test | 0  | 1  | 1  | 0  | 0  | 6.73 | 0.62 | 0\n",
    "Test |  null | null  | 0  | 1  | 0  | 7.0 | 0.67 | 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dimensionality reduction can make the modle traing and testing faster\n",
    "2. Dimensionality reduction can reduce the risk of over fitting\n",
    "3. Dimensionality reduction can save money since we use less data to build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I want to find the important features and don't worry about classification different type of data. I would like to choose PCA. Since, PCA can help me reduce the dimension and keep lots of imformation at the same time.  \n",
    "Then, if I want to recognize the different type of data and keep the shape of the data when I mapping them into a higher dimension, I will choose LLE. Since it can keep these information for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Evaluation #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“precision-recall curve” (PR curve) is a curve that describe the relationship of recall and precison. When the Precision goes up, the recall goes down and vice versa.  \n",
    "If we put the PR curve of different machine learning models together, we could compare the performance of every model easily and help us choose the best performance model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix shows us the TP, FP, FN for us. Then the Precision is calculated by TP/(TP+FP) and the recall is calculated by TP/(FN+TP). So, the PR curve can be calculated by confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve is the describe of the true positive rate and false positive rate of a model. We could see how the true positive rate change when false positive rate change.  \n",
    "What's more we could use the AUC(Area under the ROC Curve) to judge a ML model. The bigger the AUC is, the better the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix can show us the TP, TN, FP, FN for us. Then the false positive rate is calculated by FP/(TN+FP) and the recall is calculated by TP/(FN+TP). So, the ROC curve can be calculated by confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Analysis #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a1) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competation: [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)  \n",
    "I think the competation is the extend of boston dataset. It's build for data science students to practice their skills.  \n",
    "It is our job to predict the sales price for each house. For each Id in the test set, we must predict the value of the SalePrice variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b1) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa.  \n",
    "The form of the dataset is csv.  \n",
    "The out put of the submission has two column which is ID and the house price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c1) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1460 rows in training dataset and 1459 rows in testing data set. So the dataset is partitioned by 50% each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d1) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a2) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competation: [Histopathologic Cancer Detection](https://www.kaggle.com/c/histopathologic-cancer-detection)  \n",
    "In this competition, we must create an algorithm to identify metastatic cancer in small image patches taken from larger digital pathology scans.  \n",
    "The motivation of this competation is for the machine learning community to use for fun and practice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b2) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this competation, we are going to classify a large number of small pathology images. To do this, we need to predict the label of the images. We could recognize wheter a patch contains at least one pixel of tumor tissue. And where the tumor tissue is.  \n",
    "So, based on the description above, I think the input should be the image.  \n",
    "And from the sample submission, the output should be the id of the picture and the predict result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c2) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is very large and it's about 6GB data. So, I can't see the distribution of the testing and training dataset from the kanggle. However, I could know that there are 220025 unique values in the train lables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d2) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
